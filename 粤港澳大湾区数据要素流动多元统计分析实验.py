import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import warnings

warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# 1. æ•°æ®åŠ è½½
df = pd.read_csv('main_data_advanced.csv', encoding='utf-8-sig')
df_2023 = df[df['å¹´ä»½'] == 2023].copy()

print("æ•°æ®åŠ è½½å®Œæˆï¼")
print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
print(f"2023å¹´æ•°æ®: {df_2023.shape[0]}ä¸ªåŸå¸‚")

print("\nç”Ÿæˆå›¾1: è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡è¶‹åŠ¿")

plt.figure(figsize=(12, 8))
yearly_avg = df.groupby('å¹´ä»½')['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB'].mean()
plt.plot(yearly_avg.index, yearly_avg.values, marker='o', linewidth=3, markersize=10, color='royalblue')
plt.fill_between(yearly_avg.index, yearly_avg.values, alpha=0.2, color='royalblue')

plt.title('ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºè·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡å¹´å‡å˜åŒ–è¶‹åŠ¿ (2019-2023)', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('å¹´ä»½', fontsize=14)
plt.ylabel('è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡ (TB)', fontsize=14)
plt.grid(True, alpha=0.3)

# æ·»åŠ å¢é•¿ç‡æ ‡æ³¨
for i, (x, y) in enumerate(zip(yearly_avg.index, yearly_avg.values)):
    if i > 0:
        growth = (y - yearly_avg.values[i - 1]) / yearly_avg.values[i - 1] * 100
        plt.annotate(f'+{growth:.1f}%', (x, y), textcoords="offset points",
                     xytext=(0, 10), ha='center', fontsize=10, color='red')

plt.tight_layout()
plt.savefig('å›¾1_è·¨å¢ƒæ•°æ®ä¼ è¾“è¶‹åŠ¿.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nç”Ÿæˆå›¾2: å„åŸå¸‚ç ”å‘æŠ•å…¥å¯¹æ¯”")

plt.figure(figsize=(14, 8))
sorted_rd = df_2023.sort_values('ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ', ascending=True)

bars = plt.barh(sorted_rd['åŸå¸‚'], sorted_rd['ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ'],
                color=plt.cm.viridis(np.linspace(0, 1, len(sorted_rd))))

plt.title('2023å¹´ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºå„åŸå¸‚ç ”å‘ç»è´¹æŠ•å…¥å¯¹æ¯”', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('ç ”å‘ç»è´¹æŠ•å…¥ (äº¿å…ƒ)', fontsize=14)
plt.ylabel('åŸå¸‚', fontsize=14)
plt.grid(True, alpha=0.3, axis='x')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar in bars:
    width = bar.get_width()
    plt.text(width + 5, bar.get_y() + bar.get_height() / 2,
             f'{width:.1f}', va='center', fontsize=10)

plt.tight_layout()
plt.savefig('å›¾2_å„åŸå¸‚ç ”å‘æŠ•å…¥å¯¹æ¯”.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n1. ç”Ÿæˆå›¾3: ç›¸å…³æ€§çƒ­åŠ›å›¾")
plt.figure(figsize=(12, 10))

key_vars = ['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB', 'æ•°æ®ä¸­å¿ƒæœºæ¶æ•°', 'GDP_äº¿å…ƒ',
            'ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ', '5GåŸºç«™æ•°é‡', 'æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ']

corr_matrix = df[key_vars].corr()

sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',
            center=0, square=True, linewidths=1,
            cbar_kws={"shrink": 0.8, "label": "ç›¸å…³ç³»æ•°"})

plt.title('å…³é”®å˜é‡ç›¸å…³æ€§çŸ©é˜µçƒ­åŠ›å›¾', fontsize=16, fontweight='bold', pad=20)
plt.xticks(rotation=45, ha='right', fontsize=11)
plt.yticks(rotation=0, fontsize=11)

plt.tight_layout()
plt.savefig('å›¾3_ç›¸å…³æ€§çƒ­åŠ›å›¾.png', dpi=300, bbox_inches='tight')
plt.show()

# ä¿å­˜ç›¸å…³æ€§çŸ©é˜µåˆ°CSV
corr_matrix.to_csv('ç›¸å…³æ€§çŸ©é˜µ.csv', encoding='utf-8-sig')
print("ç›¸å…³æ€§çŸ©é˜µå·²ä¿å­˜è‡³: ç›¸å…³æ€§çŸ©é˜µ.csv")

print("\n2. åç›¸å…³åˆ†æï¼ˆæ§åˆ¶å˜é‡ï¼šGDP_äº¿å…ƒï¼‰")

import pingouin as pg

# é€‰æ‹©å˜é‡è¿›è¡Œåç›¸å…³åˆ†æ
partial_vars = [
    'è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB', 'ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ', 'æ•°æ®ä¸­å¿ƒæœºæ¶æ•°',
    '5GåŸºç«™æ•°é‡', 'æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ', 'GDP_äº¿å…ƒ'
]
df_partial = df_2023[partial_vars].dropna()

print("åç›¸å…³åˆ†æç»“æœï¼ˆæ§åˆ¶å˜é‡ï¼šGDP_äº¿å…ƒï¼‰")
print("=" * 80)

partial_results = []
for var in ['ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ', 'æ•°æ®ä¸­å¿ƒæœºæ¶æ•°', '5GåŸºç«™æ•°é‡', 'æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ']:
    try:
        # è®¡ç®—åç›¸å…³
        pc = pg.partial_corr(data=df_partial,
                             x='è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB',
                             y=var,
                             covar='GDP_äº¿å…ƒ',
                             method='pearson')

        # è®¡ç®—ç®€å•ç›¸å…³
        simple_r = df_partial['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB'].corr(df_partial[var])

        # ä¿å­˜ç»“æœ
        result = {
            'å˜é‡': var,
            'ç®€å•ç›¸å…³ç³»æ•°': simple_r,
            'åç›¸å…³ç³»æ•°': pc['r'].values[0],
            'på€¼': pc['p-val'].values[0],
            'æ ·æœ¬é‡': pc['n'].values[0],
            'å˜åŒ–': pc['r'].values[0] - simple_r
        }
        partial_results.append(result)

        # æ‰“å°ç»“æœ
        print(f"ç›®æ ‡å˜é‡ï¼šè·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB ä¸ {var}")
        print(f"  ç®€å•ç›¸å…³ç³»æ•°: {simple_r:.3f}")
        print(f"  åç›¸å…³ç³»æ•°:   {pc['r'].values[0]:.3f}")
        print(f"  på€¼:         {pc['p-val'].values[0]:.4f}")
        print(f"  æ ·æœ¬é‡:       {pc['n'].values[0]}")
        print(f"  å˜åŒ–å·®å¼‚:     {pc['r'].values[0] - simple_r:+.3f}")
        print("-" * 60)

    except Exception as e:
        print(f"è®¡ç®—{var}åç›¸å…³æ—¶å‡ºé”™: {e}")

try:
    pc_extra = pg.partial_corr(data=df_partial,
                               x='ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ',
                               y='æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ',
                               covar='GDP_äº¿å…ƒ',
                               method='pearson')

    result = {
        'å˜é‡': 'ç ”å‘æŠ•å…¥~æ•°å­—ç»æµå¢åŠ å€¼',
        'ç®€å•ç›¸å…³ç³»æ•°': df_partial['ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ'].corr(df_partial['æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ']),
        'åç›¸å…³ç³»æ•°': pc_extra['r'].values[0],
        'på€¼': pc_extra['p-val'].values[0],
        'æ ·æœ¬é‡': pc_extra['n'].values[0],
        'å˜åŒ–': pc_extra['r'].values[0] - df_partial['ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ'].corr(
            df_partial['æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ'])
    }
    partial_results.append(result)

    print(f"\né¢å¤–åˆ†æï¼šç ”å‘æŠ•å…¥ ~ æ•°å­—ç»æµå¢åŠ å€¼ï¼ˆæ§åˆ¶GDPï¼‰")
    print(f"  ç®€å•ç›¸å…³ç³»æ•°: {result['ç®€å•ç›¸å…³ç³»æ•°']:.3f}")
    print(f"  åç›¸å…³ç³»æ•°:   {result['åç›¸å…³ç³»æ•°']:.3f}")
    print(f"  på€¼:         {result['på€¼']:.4f}")

except Exception as e:
    print(f"è®¡ç®—é¢å¤–åç›¸å…³æ—¶å‡ºé”™: {e}")

# ä¿å­˜åç›¸å…³ç»“æœ
partial_df = pd.DataFrame(partial_results)
partial_df.to_csv('åç›¸å…³åˆ†æç»“æœ.csv', index=False, encoding='utf-8-sig')
print("\nåç›¸å…³åˆ†æç»“æœå·²ä¿å­˜è‡³: åç›¸å…³åˆ†æç»“æœ.csv")

print("\n1. KMOä¸Bartlettçƒå½¢æ£€éªŒ")
from factor_analyzer.factor_analyzer import calculate_kmo
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity

# é€‰æ‹©å› å­åˆ†æå˜é‡ï¼ˆä¸ä¸»æˆåˆ†åˆ†æä¸€è‡´ï¼‰
factor_vars = ['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB', 'æ•°æ®ä¸­å¿ƒæœºæ¶æ•°', 'GDP_äº¿å…ƒ',
               'æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ', 'ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ', '5GåŸºç«™æ•°é‡']

factor_data = df_2023[factor_vars].dropna()

# è®¡ç®—KMOå€¼
kmo_all, kmo_model = calculate_kmo(factor_data)
print(f"KMOæ£€éªŒå€¼: {kmo_model:.3f}")

# åˆ¤æ–­KMOå€¼
if kmo_model >= 0.9:
    kmo_judge = "éå¸¸é€‚åˆ"
elif kmo_model >= 0.8:
    kmo_judge = "å¾ˆé€‚åˆ"
elif kmo_model >= 0.7:
    kmo_judge = "é€‚åˆ"
elif kmo_model >= 0.6:
    kmo_judge = "å‹‰å¼ºé€‚åˆ"
elif kmo_model >= 0.5:
    kmo_judge = "ä¸å¤ªé€‚åˆ"
else:
    kmo_judge = "å®Œå…¨ä¸é€‚åˆ"

print(f"KMOåˆ¤æ–­: {kmo_judge} (â‰¥0.7ä¸ºé€‚åˆå› å­åˆ†æ)")

# Bartlettçƒå½¢æ£€éªŒ
chi_square_value, p_value = calculate_bartlett_sphericity(factor_data)
print(f"\nBartlettçƒå½¢æ£€éªŒ:")
print(f"  è¿‘ä¼¼å¡æ–¹å€¼: {chi_square_value:.2f}")
print(f"  è‡ªç”±åº¦: {len(factor_vars) * (len(factor_vars) - 1) // 2:.0f}")
print(f"  æ˜¾è‘—æ€§på€¼: {p_value:.6f}")
if p_value < 0.001:
    print(f"  æ£€éªŒç»“è®º: æå…¶æ˜¾è‘— (p<0.001)ï¼Œå¼ºçƒˆæ‹’ç»å˜é‡ç‹¬ç«‹å‡è®¾")
elif p_value < 0.05:
    print(f"  æ£€éªŒç»“è®º: æ˜¾è‘— (p<0.05)ï¼Œæ‹’ç»å˜é‡ç‹¬ç«‹å‡è®¾")
else:
    print(f"  æ£€éªŒç»“è®º: ä¸æ˜¾è‘—ï¼Œä¸èƒ½æ‹’ç»å˜é‡ç‹¬ç«‹å‡è®¾")

# ä¿å­˜æ£€éªŒç»“æœ
kmo_bartlett_result = pd.DataFrame({
    'æ£€éªŒæŒ‡æ ‡': ['KMOå€¼', 'Bartlettå¡æ–¹å€¼', 'è‡ªç”±åº¦', 'på€¼'],
    'æ•°å€¼': [kmo_model, chi_square_value, len(factor_vars) * (len(factor_vars) - 1) // 2, p_value],
    'åˆ¤æ–­æ ‡å‡†': ['â‰¥0.7ä¸ºé€‚åˆ', 'p<0.05ä¸ºæ˜¾è‘—', '', 'p<0.05æ‹’ç»åŸå‡è®¾'],
    'ç»“è®º': [kmo_judge, 'æå…¶æ˜¾è‘—' if p_value < 0.001 else ('æ˜¾è‘—' if p_value < 0.05 else 'ä¸æ˜¾è‘—'), '',
             'æ•°æ®å­˜åœ¨ç›¸å…³æ€§' if p_value < 0.05 else 'æ•°æ®ç‹¬ç«‹']
})
kmo_bartlett_result.to_csv('KMO_Bartlettæ£€éªŒç»“æœ.csv', index=False, encoding='utf-8-sig')
print("\nKMOä¸Bartlettæ£€éªŒç»“æœå·²ä¿å­˜è‡³: KMO_Bartlettæ£€éªŒç»“æœ.csv")

print("\n2. ç”Ÿæˆå›¾4: PCAæ–¹å·®è§£é‡Šç‡å›¾")
plt.figure(figsize=(10, 8))

pca_vars = factor_vars  # ä½¿ç”¨ç›¸åŒçš„å˜é‡

pca_data = df_2023[pca_vars].copy()
scaler = StandardScaler()
pca_data_scaled = scaler.fit_transform(pca_data)

pca = PCA()
pca.fit(pca_data_scaled)
explained_var = pca.explained_variance_ratio_
cumulative_var = np.cumsum(explained_var)

components = range(1, len(explained_var) + 1)

# åˆ›å»ºåŒYè½´
fig, ax1 = plt.subplots(figsize=(10, 8))

# æ¡å½¢å›¾ï¼šå•ä¸ªä¸»æˆåˆ†è§£é‡Šç‡
bars = ax1.bar(components, explained_var, alpha=0.6, color='skyblue', label='å•ä¸ªæˆåˆ†è§£é‡Šç‡')
ax1.set_xlabel('ä¸»æˆåˆ†', fontsize=14)
ax1.set_ylabel('å•ä¸ªæˆåˆ†è§£é‡Šç‡', fontsize=14, color='skyblue')
ax1.tick_params(axis='y', labelcolor='skyblue')
ax1.set_xticks(components)

# æŠ˜çº¿å›¾ï¼šç´¯è®¡è§£é‡Šç‡
ax2 = ax1.twinx()
line = ax2.plot(components, cumulative_var, 'r-', marker='o', linewidth=3,
                markersize=8, label='ç´¯è®¡è§£é‡Šç‡')
ax2.set_ylabel('ç´¯è®¡è§£é‡Šç‡', fontsize=14, color='red')
ax2.tick_params(axis='y', labelcolor='red')
ax2.set_ylim([0, 1.1])

# æ·»åŠ é˜ˆå€¼çº¿
ax2.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, linewidth=2, label='80%é˜ˆå€¼')

plt.title('PCAæ–¹å·®è§£é‡Šç‡åˆ†æï¼ˆç¢çŸ³å›¾ï¼‰', fontsize=16, fontweight='bold', pad=20)

# åˆå¹¶å›¾ä¾‹
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')

plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig('å›¾4_PCAæ–¹å·®è§£é‡Šç‡.png', dpi=300, bbox_inches='tight')
plt.show()

# è¾“å‡ºä¸»æˆåˆ†è¯¦ç»†ä¿¡æ¯
print("\nä¸»æˆåˆ†è¯¦ç»†ä¿¡æ¯:")
for i, (ev, evr, cv) in enumerate(zip(pca.explained_variance_, explained_var, cumulative_var), 1):
    print(f"  ä¸»æˆåˆ†{i}: ç‰¹å¾å€¼={ev:.3f}, è§£é‡Šæ–¹å·®={evr * 100:.1f}%, ç´¯è®¡è§£é‡Šæ–¹å·®={cv * 100:.1f}%")

# ç¡®å®šæå–çš„ä¸»æˆåˆ†æ•°é‡ï¼ˆç‰¹å¾å€¼>1ä¸”ç´¯è®¡è´¡çŒ®ç‡>80%ï¼‰
n_components = sum(pca.explained_variance_ > 1)
print(f"\næ ¹æ®ç‰¹å¾å€¼>1åŸåˆ™ï¼Œæå–ä¸»æˆåˆ†æ•°é‡: {n_components}")
print(f"å‰{n_components}ä¸ªä¸»æˆåˆ†ç´¯è®¡è§£é‡Šæ–¹å·®: {cumulative_var[n_components - 1] * 100:.1f}%")

print("\nã€ç²¾ç¡®PCAè¾“å‡ºã€‘ä¸»æˆåˆ†ç‰¹å¾å€¼ä¸è´¡çŒ®ç‡è¡¨")
print("=" * 50)

# è·å–ç²¾ç¡®çš„ç‰¹å¾å€¼ï¼ˆè§£é‡Šæ–¹å·®ï¼‰
explained_variance = pca.explained_variance_
# è·å–ç²¾ç¡®çš„æ–¹å·®è´¡çŒ®ç‡
explained_variance_ratio = pca.explained_variance_ratio_
# è®¡ç®—ç´¯è®¡è´¡çŒ®ç‡
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

# åˆ›å»ºå¹¶æ˜¾ç¤ºç²¾ç¡®çš„è¡¨æ ¼
pca_details_df = pd.DataFrame({
    'ä¸»æˆåˆ†': [f'PC{i + 1}' for i in range(len(explained_variance))],
    'ç‰¹å¾å€¼': explained_variance,
    'æ–¹å·®è´¡çŒ®ç‡(%)': explained_variance_ratio * 100,
    'ç´¯è®¡è´¡çŒ®ç‡(%)': cumulative_variance_ratio * 100
})

print(pca_details_df.round(3).to_string(index=False))
pca_details_df.to_csv('PCA_ç‰¹å¾å€¼ä¸è´¡çŒ®ç‡_ç²¾ç¡®è¡¨.csv', index=False, encoding='utf-8-sig')


n_components_to_show = 3
print(f"\nå‰{n_components_to_show}ä¸ªä¸»æˆåˆ†çš„è¯¦ç»†æƒ…å†µ:")
for i in range(min(n_components_to_show, len(explained_variance))):
    print(f"  PC{i + 1}: ç‰¹å¾å€¼={explained_variance[i]:.3f}, ",
          f"è´¡çŒ®ç‡={explained_variance_ratio[i] * 100:.1f}%, ",
          f"ç´¯è®¡={cumulative_variance_ratio[i] * 100:.1f}%")

print("\n3. ä¸»æˆåˆ†è½½è·çŸ©é˜µ")

# è·å–ä¸»æˆåˆ†è½½è·çŸ©é˜µï¼ˆç‰¹å¾å‘é‡ä¹˜ä»¥ç‰¹å¾å€¼å¹³æ–¹æ ¹ï¼‰
pca_components = pca.components_.T * np.sqrt(pca.explained_variance_)

# åˆ›å»ºè½½è·è¡¨
loadings_df = pd.DataFrame(
    pca_components[:, :n_components],
    index=pca_vars,
    columns=[f'PC{i + 1}' for i in range(n_components)]
)

# ç®€åŒ–çš„å˜é‡åç”¨äºæ˜¾ç¤º
var_names_simple = [v.split('_')[0] if '_' in v else v for v in pca_vars]
loadings_display = pd.DataFrame(
    pca_components[:, :n_components],
    index=var_names_simple,
    columns=[f'PC{i + 1}' for i in range(n_components)]
)

print("\nä¸»æˆåˆ†è½½è·çŸ©é˜µï¼ˆå‰3ä¸ªä¸»æˆåˆ†ï¼‰:")
print(loadings_display.round(3))

# ä¿å­˜åˆ°CSV
loadings_df.to_csv('ä¸»æˆåˆ†è½½è·çŸ©é˜µ.csv', encoding='utf-8-sig')
print("\nä¸»æˆåˆ†è½½è·çŸ©é˜µå·²ä¿å­˜è‡³: ä¸»æˆåˆ†è½½è·çŸ©é˜µ.csv")

# å¯¹ä¸»æˆåˆ†è¿›è¡Œå‘½åè§£é‡Šï¼ˆåŸºäºè½½è·ç»å¯¹å€¼>0.7ï¼‰
print("\nä¸»æˆåˆ†å‘½åè§£é‡Šï¼ˆåŸºäºè½½è·ç»å¯¹å€¼>0.7ï¼‰:")
for i in range(min(3, n_components)):
    pc_num = i + 1
    high_loadings = loadings_display.iloc[:, i].abs().nlargest(3)
    print(f"\n  PC{pc_num} (è§£é‡Šæ–¹å·® {explained_var[i] * 100:.1f}%):")
    for var_name, loading in high_loadings.items():
        original_var = pca_vars[var_names_simple.index(var_name)]
        print(f"    â€¢ {var_name}: {loadings_display.loc[var_name, f'PC{pc_num}']:.3f}")

print("\n4. ç”Ÿæˆå›¾5: PCAæ•£ç‚¹å›¾")

plt.figure(figsize=(12, 10))

pca_result = pca.transform(pca_data_scaled)

scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1],
                      s=200, alpha=0.7, edgecolors='black', linewidth=1.5,
                      c=range(len(df_2023)), cmap='viridis')

# æ·»åŠ åŸå¸‚æ ‡ç­¾
for i, city in enumerate(df_2023['åŸå¸‚']):
    plt.annotate(city, (pca_result[i, 0], pca_result[i, 1]),
                 fontsize=11, alpha=0.8,
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))

plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)

plt.xlabel(f'ç¬¬ä¸€ä¸»æˆåˆ† PC1 ({explained_var[0] * 100:.1f}%)', fontsize=14)
plt.ylabel(f'ç¬¬äºŒä¸»æˆåˆ† PC2 ({explained_var[1] * 100:.1f}%)', fontsize=14)
plt.title('åŸå¸‚åœ¨PCAä¸»æˆåˆ†ç©ºé—´ä¸­çš„åˆ†å¸ƒ', fontsize=16, fontweight='bold', pad=20)
plt.grid(True, alpha=0.3)

# æ·»åŠ é¢œè‰²æ¡
cbar = plt.colorbar(scatter)
cbar.set_label('åŸå¸‚åºå·', fontsize=12)

plt.tight_layout()
plt.savefig('å›¾5_PCAæ•£ç‚¹å›¾.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n5. å› å­åˆ†æå°è¯•ï¼ˆKMO=0.455ï¼Œä¸é€‚åˆï¼‰")

# æ‰§è¡Œå› å­åˆ†æ
fa_vars = factor_vars + ['æ•°æ®äº¤æ˜“é¢_äº¿å…ƒ', 'é‡‘èç§‘æŠ€äº¤æ˜“è§„æ¨¡_äº¿å…ƒ']
fa_data = df_2023[fa_vars].copy().dropna()

# æ£€æŸ¥æ•°æ®æ˜¯å¦é€‚åˆå› å­åˆ†æ
try:
    fa_kmo_all, fa_kmo_model = calculate_kmo(fa_data)
    print(f"å› å­åˆ†æKMOå€¼: {fa_kmo_model:.3f}")

    if fa_kmo_model >= 0.7:
        print("æ•°æ®é€‚åˆè¿›è¡Œå› å­åˆ†æ")
        # ... (åŸæœ‰å› å­åˆ†æä»£ç ï¼Œä½†KMO=0.455ä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ)
    else:
        print(f"KMOå€¼{fa_kmo_model:.3f}ä½äº0.7ï¼Œæ•°æ®ä¸é€‚åˆå› å­åˆ†æï¼Œè·³è¿‡æ­£å¼åˆ†æ")

except Exception as e:
    print(f"å› å­åˆ†æå‡ºé”™: {e}")


print("\n1. ç”Ÿæˆå›¾7.1: è½®å»“ç³»æ•°ç¡®å®šæœ€ä¼˜èšç±»æ•°")
plt.figure(figsize=(10, 6))
# ä½¿ç”¨PCAç»“æœè¿›è¡Œèšç±»
X_cluster = pca_result[:, :2]  # ä½¿ç”¨å‰ä¸¤ä¸ªä¸»æˆåˆ†

# ç¡®å®šæœ€ä¼˜èšç±»æ•°
sil_scores = []
k_range = range(2, 8)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_cluster)
    sil_score = silhouette_score(X_cluster, labels)
    sil_scores.append(sil_score)

optimal_k = k_range[np.argmax(sil_scores)]

# ç»˜åˆ¶è½®å»“ç³»æ•°å›¾
plt.plot(k_range, sil_scores, 'bo-', linewidth=2, markersize=8)
plt.xlabel('èšç±»æ•°é‡ (K)', fontsize=14)
plt.ylabel('è½®å»“ç³»æ•°', fontsize=14)
plt.title('è½®å»“ç³»æ•°æ³•ç¡®å®šæœ€ä¼˜èšç±»æ•°', fontsize=16, fontweight='bold', pad=20)
plt.grid(True, alpha=0.3)
plt.axvline(x=optimal_k, color='r', linestyle='--', label=f'æœ€ä¼˜K={optimal_k}')
plt.legend()

plt.tight_layout()
plt.savefig('å›¾7.1_è½®å»“ç³»æ•°ç¡®å®šæœ€ä¼˜èšç±»æ•°.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"æœ€ä¼˜èšç±»æ•°: {optimal_k} (è½®å»“ç³»æ•°: {max(sil_scores):.3f})")

print("\n2. ç”Ÿæˆå›¾6: K-meansèšç±»ç»“æœ")

plt.figure(figsize=(12, 10))

# æ‰§è¡ŒK-meansèšç±»
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df_2023['èšç±»æ ‡ç­¾'] = kmeans.fit_predict(X_cluster)


def get_cluster_name(cluster_id, cities):
    if cluster_id == 0:
        return "å¤–å›´å‘å±•å‹"
    elif cluster_id == 1:
        if cities == ['æ·±åœ³']:
            return "åˆ›æ–°å¼•é¢†å‹"
        else:
            return "æ ¸å¿ƒå¼•é¢†å‹"
    elif cluster_id == 2:
        if 'å¹¿å·' in cities and 'é¦™æ¸¯' in cities:
            return "æ¢çº½æ”¯æ’‘å‹"
        else:
            return f"ç±»åˆ«{cluster_id}"
    else:
        return f"ç±»åˆ«{cluster_id}"
# ç»˜åˆ¶èšç±»ç»“æœ
colors = plt.cm.Set3(np.linspace(0, 1, optimal_k))
# è·å–æ¯ä¸ªèšç±»çš„åŸå¸‚åˆ—è¡¨
cluster_cities_map = {}
for cluster_id in range(optimal_k):
    cluster_cities = df_2023[df_2023['èšç±»æ ‡ç­¾'] == cluster_id]['åŸå¸‚'].tolist()
    cluster_cities_map[cluster_id] = cluster_cities

# æŒ‰ç±»åˆ«åç§°æ’åºç»˜åˆ¶
for cluster_id in range(optimal_k):
    cluster_data = pca_result[df_2023['èšç±»æ ‡ç­¾'] == cluster_id]
    cluster_name = get_cluster_name(cluster_id, cluster_cities_map[cluster_id])
    plt.scatter(cluster_data[:, 0], cluster_data[:, 1],
                s=200, alpha=0.7, edgecolors='black', linewidth=1.5,
                color=colors[cluster_id], label=cluster_name)

# æ ‡è®°èšç±»ä¸­å¿ƒ
centers = kmeans.cluster_centers_[:, :2]
plt.scatter(centers[:, 0], centers[:, 1],
            c='red', marker='X', s=300, alpha=0.9, linewidth=3, label='èšç±»ä¸­å¿ƒ')

# æ·»åŠ åŸå¸‚æ ‡ç­¾
for i, city in enumerate(df_2023['åŸå¸‚']):
    plt.annotate(city, (pca_result[i, 0], pca_result[i, 1]),
                 fontsize=10, alpha=0.8)

plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)

plt.xlabel('ç¬¬ä¸€ä¸»æˆåˆ† PC1', fontsize=14)
plt.ylabel('ç¬¬äºŒä¸»æˆåˆ† PC2', fontsize=14)
plt.title(f'K-meansèšç±»ç»“æœ (K={optimal_k})', fontsize=16, fontweight='bold', pad=20)
plt.legend(loc='best', fontsize=11)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('å›¾6_Kmeansèšç±»ç»“æœ.png', dpi=300, bbox_inches='tight')
plt.show()

# è¾“å‡ºèšç±»ç»“æœ
print("\nèšç±»åˆ†ç»„ç»“æœ:")
cluster_summary = []
for cluster_id in range(optimal_k):
    cluster_cities = df_2023[df_2023['èšç±»æ ‡ç­¾'] == cluster_id]['åŸå¸‚'].tolist()
    cluster_size = len(cluster_cities)

    # ä¸ºæ¯ä¸ªèšç±»å‘½å
    cluster_name = get_cluster_name(cluster_id, cluster_cities)

    cluster_summary.append({
        'ç±»åˆ«æ ‡ç­¾': cluster_id,
        'ç±»åˆ«åç§°': cluster_name,
        'åŸå¸‚æ•°é‡': cluster_size,
        'åŸå¸‚åˆ—è¡¨': ', '.join(cluster_cities)
    })
    print(f"  {cluster_name} (ç±»åˆ«{cluster_id}, {cluster_size}ä¸ªåŸå¸‚): {', '.join(cluster_cities)}")

# ä¿å­˜èšç±»ç»“æœ
cluster_df = pd.DataFrame(cluster_summary)
cluster_df.to_csv('èšç±»åˆ†æç»“æœ.csv', index=False, encoding='utf-8-sig')
print("\nèšç±»åˆ†æç»“æœå·²ä¿å­˜è‡³: èšç±»åˆ†æç»“æœ.csv")

print("\n3. ç”Ÿæˆè¡¨7.1: èšç±»ç±»åˆ«å…³é”®æŒ‡æ ‡å¯¹æ¯”åˆ†æ")

# è®¡ç®—å„ç±»åˆ«å‡å€¼å¯¹æ¯”
key_indicators = ['GDP_äº¿å…ƒ', 'ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ', 'è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB',
                  'æ•°å­—ç»æµå GDPæ¯”é‡_%', 'æ•°æ®ä¸­å¿ƒæœºæ¶æ•°', '5GåŸºç«™æ•°é‡']

# æŒ‰èšç±»æ ‡ç­¾åˆ†ç»„è®¡ç®—
group_stats = df_2023.groupby('èšç±»æ ‡ç­¾')[key_indicators].mean().round(1)

# åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
comparison_data = []
for cluster_id in sorted(group_stats.index):
    cluster_name = get_cluster_name(cluster_id, cluster_cities_map.get(cluster_id, []))
    cluster_values = group_stats.loc[cluster_id]

    # è®¡ç®—ä¸æ‰€æœ‰åŸå¸‚å‡å€¼çš„å·®å¼‚å€æ•°
    overall_mean = df_2023[key_indicators].mean()
    diff_ratio = (cluster_values / overall_mean).round(2)

    comparison_data.append({
        'ç±»åˆ«': cluster_name,
        'GDPï¼ˆäº¿å…ƒï¼‰': f"{cluster_values['GDP_äº¿å…ƒ']:.1f} ({diff_ratio['GDP_äº¿å…ƒ']}å€)",
        'ç ”å‘æŠ•å…¥ï¼ˆäº¿å…ƒï¼‰': f"{cluster_values['ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ']:.1f} ({diff_ratio['ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ']}å€)",
        'æ•°æ®æµé‡ï¼ˆTBï¼‰': f"{cluster_values['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB']:.1f} ({diff_ratio['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB']}å€)",
        'æ•°å­—ç»æµå æ¯”ï¼ˆ%ï¼‰': f"{cluster_values['æ•°å­—ç»æµå GDPæ¯”é‡_%']:.1f} ({diff_ratio['æ•°å­—ç»æµå GDPæ¯”é‡_%']}å€)",
        'æ•°æ®ä¸­å¿ƒæœºæ¶æ•°': f"{cluster_values['æ•°æ®ä¸­å¿ƒæœºæ¶æ•°']:.1f} ({diff_ratio['æ•°æ®ä¸­å¿ƒæœºæ¶æ•°']}å€)",
        '5GåŸºç«™æ•°é‡': f"{cluster_values['5GåŸºç«™æ•°é‡']:.1f} ({diff_ratio['5GåŸºç«™æ•°é‡']}å€)"
    })

comparison_df = pd.DataFrame(comparison_data)

print("\nè¡¨7.1 èšç±»ç±»åˆ«å…³é”®æŒ‡æ ‡å¯¹æ¯”ï¼ˆ2023å¹´ï¼‰")
print("=" * 100)
print(comparison_df.to_string(index=False))

# ä¿å­˜åˆ°CSVï¼ˆåªä¿å­˜æ•°å€¼éƒ¨åˆ†ï¼‰
numeric_comparison_df = group_stats.copy()
numeric_comparison_df['ç±»åˆ«åç§°'] = [get_cluster_name(i, cluster_cities_map.get(i, [])) for i in
                                     numeric_comparison_df.index]
numeric_comparison_df.reset_index(inplace=True)
numeric_comparison_df.to_csv('èšç±»ç±»åˆ«ç‰¹å¾å¯¹æ¯”è¡¨.csv', index=False, encoding='utf-8-sig')
print("\nèšç±»ç±»åˆ«ç‰¹å¾å¯¹æ¯”è¡¨å·²ä¿å­˜è‡³: èšç±»ç±»åˆ«ç‰¹å¾å¯¹æ¯”è¡¨.csv")

print("\n4. ç”Ÿæˆå›¾7.3: å¤šç±»åŸå¸‚ç¾¤åŠ¨æ€æ¼”åŒ–åˆ†æ")

# ä¸ºæ¯å¹´è®¡ç®—PCAå¾—åˆ†
pca_vars_for_history = ['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB', 'æ•°æ®ä¸­å¿ƒæœºæ¶æ•°', 'GDP_äº¿å…ƒ',
                        'æ•°å­—ç»æµæ ¸å¿ƒäº§ä¸šå¢åŠ å€¼_äº¿å…ƒ', 'ç ”å‘ç»è´¹æŠ•å…¥_äº¿å…ƒ', '5GåŸºç«™æ•°é‡']

yearly_pc1_scores = []

for year in sorted(df['å¹´ä»½'].unique()):
    df_year = df[df['å¹´ä»½'] == year].copy()

    # æ ‡å‡†åŒ–
    scaler_year = StandardScaler()
    pca_data_year_scaled = scaler_year.fit_transform(df_year[pca_vars_for_history])

    # ä½¿ç”¨ä¸2023å¹´ç›¸åŒçš„PCAå¯¹è±¡ï¼ˆåŸºäº2023å¹´æ•°æ®è®­ç»ƒçš„ï¼‰
    pca_year = PCA(n_components=1)
    pca_year.fit(pca_data_year_scaled)
    pc1_scores_year = pca_year.transform(pca_data_year_scaled)

    # ä¸ºæ¯ä¸ªåŸå¸‚è®°å½•PC1å¾—åˆ†å’Œå¹´ä»½
    for idx, city in enumerate(df_year['åŸå¸‚']):
        yearly_pc1_scores.append({
            'å¹´ä»½': year,
            'åŸå¸‚': city,
            'PC1_å¾—åˆ†': pc1_scores_year[idx][0]
        })

# è½¬æ¢ä¸ºDataFrame
df_pc1_history = pd.DataFrame(yearly_pc1_scores)

# å°†2023å¹´çš„èšç±»æ ‡ç­¾æ˜ å°„åˆ°å†å²æ•°æ®
df_2023_labels = df_2023[['åŸå¸‚', 'èšç±»æ ‡ç­¾']].copy()
df_pc1_history = pd.merge(df_pc1_history, df_2023_labels, on='åŸå¸‚', how='left')

# ä¸ºæ¯ä¸ªèšç±»æ·»åŠ ç±»åˆ«åç§°
df_pc1_history['ç±»åˆ«åç§°'] = df_pc1_history.apply(
    lambda row: get_cluster_name(row['èšç±»æ ‡ç­¾'],
                                 df_2023[df_2023['èšç±»æ ‡ç­¾'] == row['èšç±»æ ‡ç­¾']]['åŸå¸‚'].tolist()
                                 if row['èšç±»æ ‡ç­¾'] in df_2023['èšç±»æ ‡ç­¾'].values else []),
    axis=1
)

# è®¡ç®—æ¯å¹´æ¯ç±»çš„å¹³å‡PC1å¾—åˆ†
class_yearly_avg = df_pc1_history.groupby(['å¹´ä»½', 'ç±»åˆ«åç§°'])['PC1_å¾—åˆ†'].mean().reset_index()

# ç»˜åˆ¶åŠ¨æ€æ¼”åŒ–å›¾
plt.figure(figsize=(12, 8))

# ä¸ºæ¯ä¸ªç±»åˆ«ç»˜åˆ¶è¶‹åŠ¿çº¿
colors_evolve = ['#1f77b4', '#ff7f0e', '#2ca02c']  # ä¸ºä¸‰ç±»åŸå¸‚è®¾ç½®ä¸åŒé¢œè‰²
for idx, class_name in enumerate(class_yearly_avg['ç±»åˆ«åç§°'].unique()):
    class_data = class_yearly_avg[class_yearly_avg['ç±»åˆ«åç§°'] == class_name].sort_values('å¹´ä»½')
    if len(class_data) > 0:
        plt.plot(class_data['å¹´ä»½'], class_data['PC1_å¾—åˆ†'],
                 marker='o', linewidth=2, markersize=8,
                 color=colors_evolve[idx % len(colors_evolve)], label=class_name)

plt.xlabel('å¹´ä»½', fontsize=14)
plt.ylabel('PC1å¹³å‡å¾—åˆ†', fontsize=14)
plt.title('ä¸åŒç±»åˆ«åŸå¸‚ç¾¤PC1å¾—åˆ†åŠ¨æ€æ¼”åŒ– (2019-2023)',
          fontsize=16, fontweight='bold', pad=20)
plt.grid(True, alpha=0.3)
plt.legend(title='åŸå¸‚ç±»åˆ«', fontsize=11)
plt.tight_layout()
plt.savefig('å›¾7.3_å¤šç±»åŸå¸‚ç¾¤åŠ¨æ€æ¼”åŒ–.png', dpi=300, bbox_inches='tight')
plt.show()

print("âœ… å›¾7.3 'å¤šç±»åŸå¸‚ç¾¤åŠ¨æ€æ¼”åŒ–' å·²ç”Ÿæˆ")

# è®¡ç®—æ¼”åŒ–è¶‹åŠ¿
print("\nåŠ¨æ€æ¼”åŒ–è¶‹åŠ¿åˆ†æ:")
for class_name in class_yearly_avg['ç±»åˆ«åç§°'].unique():
    class_data = class_yearly_avg[class_yearly_avg['ç±»åˆ«åç§°'] == class_name].sort_values('å¹´ä»½')
    if len(class_data) >= 2:
        initial_score = class_data.iloc[0]['PC1_å¾—åˆ†']
        final_score = class_data.iloc[-1]['PC1_å¾—åˆ†']
        change = final_score - initial_score
        change_percent = (change / abs(initial_score) * 100) if initial_score != 0 else 0
        print(f"  {class_name}: {initial_score:.3f} â†’ {final_score:.3f}, å˜åŒ–: {change:+.3f} ({change_percent:+.1f}%)")

print("\n5. åˆ¤åˆ«åˆ†æéªŒè¯")

# å‡†å¤‡æ•°æ®ï¼šä½¿ç”¨PCAå¾—åˆ†ä½œä¸ºç‰¹å¾ï¼Œèšç±»æ ‡ç­¾ä½œä¸ºç›®æ ‡
X = pca_result[:, :2]  # å‰ä¸¤ä¸ªä¸»æˆåˆ†
y = df_2023['èšç±»æ ‡ç­¾']

# çº¿æ€§åˆ¤åˆ«åˆ†æ
try:
    lda = LinearDiscriminantAnalysis()
    lda.fit(X, y)

    # é¢„æµ‹å¹¶è¯„ä¼°
    y_pred = lda.predict(X)

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = np.mean(y_pred == y)
    print(f"åˆ¤åˆ«åˆ†æå‡†ç¡®ç‡: {accuracy:.1%}")

    # åˆ›å»ºæ··æ·†çŸ©é˜µ
    from sklearn.metrics import confusion_matrix, classification_report

    cm = confusion_matrix(y, y_pred)

    print("\næ··æ·†çŸ©é˜µ:")
    print(cm)

    print("\nåˆ†ç±»æŠ¥å‘Š:")
    # è·å–ç±»åˆ«åç§°åˆ—è¡¨
    target_names = [get_cluster_name(i, cluster_cities_map.get(i, [])) for i in sorted(y.unique())]
    print(classification_report(y, y_pred, target_names=target_names))

    # ç‰¹å¾é‡è¦æ€§ï¼ˆåˆ¤åˆ«å‡½æ•°çš„ç³»æ•°ï¼‰
    print("\nåˆ¤åˆ«å‡½æ•°ç³»æ•° (ç‰¹å¾é‡è¦æ€§):")
    n_classes = len(lda.coef_)
    for i in range(n_classes):
        class_name = get_cluster_name(i, cluster_cities_map.get(i, []))
        feature_importance = pd.DataFrame({
            'ç‰¹å¾': ['PC1', 'PC2'],
            'ç³»æ•°': lda.coef_[i],
            'ç³»æ•°ç»å¯¹å€¼': np.abs(lda.coef_[i]),
            'ç›¸å¯¹é‡è¦æ€§(%)': (np.abs(lda.coef_[i]) / np.abs(lda.coef_[i]).sum() * 100).round(1)
        })
        print(f"\nåˆ¤åˆ«å‡½æ•°{chr(65 + i)} (å¯¹åº”{class_name}):")
        print(feature_importance.to_string(index=False))

    # ä¿å­˜åˆ¤åˆ«åˆ†æç»“æœ
    discriminant_results = {
        'å‡†ç¡®ç‡': accuracy,
        'ç±»åˆ«æ•°é‡': optimal_k,
    }
    # æ·»åŠ æ¯ä¸ªåˆ¤åˆ«å‡½æ•°çš„ç³»æ•°
    for i in range(n_classes):
        discriminant_results[f'åˆ¤åˆ«å‡½æ•°{i + 1}_ç³»æ•°_PC1'] = lda.coef_[i][0]
        discriminant_results[f'åˆ¤åˆ«å‡½æ•°{i + 1}_ç³»æ•°_PC2'] = lda.coef_[i][1]

    pd.DataFrame([discriminant_results]).to_csv('åˆ¤åˆ«åˆ†æç»“æœ.csv', index=False, encoding='utf-8-sig')
    print("\nåˆ¤åˆ«åˆ†æç»“æœå·²ä¿å­˜è‡³: åˆ¤åˆ«åˆ†æç»“æœ.csv")

except Exception as e:
    print(f"åˆ¤åˆ«åˆ†æå‡ºé”™: {e}")
    print("è·³è¿‡åˆ¤åˆ«åˆ†æéƒ¨åˆ†...")

# ç»§ç»­ç”Ÿæˆå…¶ä»–å›¾è¡¨...
print("\nç”Ÿæˆå›¾9: å¤åˆå¹´å¢é•¿ç‡åˆ†æ")
plt.figure(figsize=(14, 10))

# è®¡ç®—å„åŸå¸‚CAGR
cagr_results = []
for city in df['åŸå¸‚'].unique():
    city_df = df[df['åŸå¸‚'] == city].sort_values('å¹´ä»½')
    if len(city_df) >= 2:
        initial = city_df['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB'].iloc[0]
        final = city_df['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB'].iloc[-1]
        years = len(city_df) - 1
        if initial > 0:
            cagr = (final / initial) ** (1 / years) - 1
        else:
            cagr = 0
        cagr_results.append({'åŸå¸‚': city, 'CAGR': cagr * 100})

cagr_df = pd.DataFrame(cagr_results).sort_values('CAGR', ascending=True)  # å‡åº

# æ ¹æ®èšç±»ç±»åˆ«ç€è‰²
cagr_df = pd.merge(cagr_df, df_2023[['åŸå¸‚', 'èšç±»æ ‡ç­¾']], on='åŸå¸‚', how='left')
cagr_df['ç±»åˆ«åç§°'] = cagr_df.apply(
    lambda row: get_cluster_name(row['èšç±»æ ‡ç­¾'],
                                 df_2023[df_2023['èšç±»æ ‡ç­¾'] == row['èšç±»æ ‡ç­¾']]['åŸå¸‚'].tolist()
                                 if row['èšç±»æ ‡ç­¾'] in df_2023['èšç±»æ ‡ç­¾'].values else []),
    axis=1
)

# ç»˜åˆ¶CAGRå›¾
fig, ax = plt.subplots(figsize=(14, 10))
colors_map = {'å¤–å›´å‘å±•å‹': '#1f77b4', 'åˆ›æ–°å¼•é¢†å‹': '#ff7f0e', 'æ¢çº½æ”¯æ’‘å‹': '#2ca02c'}
bars = []
for i, (city, cagr, class_name) in enumerate(zip(cagr_df['åŸå¸‚'], cagr_df['CAGR'], cagr_df['ç±»åˆ«åç§°'])):
    color = colors_map.get(class_name, 'gray')
    bar = ax.barh(i, cagr, color=color, edgecolor='black', linewidth=1)
    bars.append(bar[0])

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    color_text = 'red' if cagr < 0 else 'green'
    ax.text(cagr + (0.5 if cagr >= 0 else -3), i,
            f'{cagr:.1f}%', va='center', fontsize=10, color=color_text, fontweight='bold')

ax.set_yticks(range(len(cagr_df)))
ax.set_yticklabels(cagr_df['åŸå¸‚'])
ax.axvline(x=0, color='black', linewidth=1)
ax.set_xlabel('å¤åˆå¹´å¢é•¿ç‡ (%)', fontsize=14)
ax.set_ylabel('åŸå¸‚', fontsize=14)
ax.set_title('å„åŸå¸‚è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡å¤åˆå¹´å¢é•¿ç‡(CAGR) 2019-2023 (æŒ‰èšç±»ç±»åˆ«ç€è‰²)',
             fontsize=16, fontweight='bold', pad=20)
ax.grid(True, alpha=0.3, axis='x')

# æ·»åŠ å›¾ä¾‹
from matplotlib.patches import Patch

legend_elements = [Patch(facecolor=color, label=label) for label, color in colors_map.items()]
ax.legend(handles=legend_elements, loc='upper right', title='åŸå¸‚ç±»åˆ«')

plt.tight_layout()
plt.savefig('å›¾9_å¤åˆå¹´å¢é•¿ç‡åˆ†æ.png', dpi=300, bbox_inches='tight')
plt.show()

# ä¿å­˜CAGRç»“æœ
cagr_df.sort_values('CAGR', ascending=False, inplace=True)
cagr_df.to_csv('å¤åˆå¹´å¢é•¿ç‡åˆ†æ.csv', index=False, encoding='utf-8-sig')
print("å¤åˆå¹´å¢é•¿ç‡åˆ†æç»“æœå·²ä¿å­˜è‡³: å¤åˆå¹´å¢é•¿ç‡åˆ†æ.csv")

print("\nç”Ÿæˆå›¾10: å‘å±•è·¯å¾„å›¾")
plt.figure(figsize=(12, 10))

# é€‰æ‹©ä»£è¡¨æ€§åŸå¸‚ï¼ˆæ¯ä¸ªç±»åˆ«é€‰1-2ä¸ªï¼‰
representative_cities = []
for cluster_id in range(optimal_k):
    cities_in_cluster = df_2023[df_2023['èšç±»æ ‡ç­¾'] == cluster_id]['åŸå¸‚'].tolist()
    # æ¯ä¸ªç±»åˆ«é€‰æ‹©1-2ä¸ªä»£è¡¨æ€§åŸå¸‚
    if len(cities_in_cluster) > 0:
        representative_cities.append(cities_in_cluster[0])
        if len(cities_in_cluster) > 1 and len(representative_cities) < 6:
            representative_cities.append(cities_in_cluster[1])

colors_path = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']

for idx, city in enumerate(representative_cities):
    city_df = df[df['åŸå¸‚'] == city].sort_values('å¹´ä»½')
    if len(city_df) > 0:
        # è®¡ç®—æ•°æ®å¯†åº¦ï¼ˆæ•°æ®æµé‡/GDPï¼‰
        data_density = city_df['è·¨å¢ƒæ•°æ®ä¼ è¾“æ€»é‡_TB'] / city_df['GDP_äº¿å…ƒ'] * 1000
        digital_share = city_df['æ•°å­—ç»æµå GDPæ¯”é‡_%']

        # è·å–è¯¥åŸå¸‚æ‰€å±ç±»åˆ«
        city_class = df_2023[df_2023['åŸå¸‚'] == city]['èšç±»æ ‡ç­¾'].values[0]
        class_name = get_cluster_name(city_class,
                                      df_2023[df_2023['èšç±»æ ‡ç­¾'] == city_class]['åŸå¸‚'].tolist())

        # ç»˜åˆ¶è·¯å¾„
        plt.plot(data_density, digital_share, marker='o', linewidth=2.5,
                 markersize=8, color=colors_path[idx % len(colors_path)],
                 label=f'{city} ({class_name})')

        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        plt.scatter(data_density.iloc[0], digital_share.iloc[0],
                    s=100, color='red', zorder=5, marker='s',
                    label=f'{city}èµ·ç‚¹' if idx == 0 else "")
        plt.scatter(data_density.iloc[-1], digital_share.iloc[-1],
                    s=100, color='green', zorder=5, marker='^',
                    label=f'{city}ç»ˆç‚¹' if idx == 0 else "")

        # æ·»åŠ å¹´ä»½æ ‡ç­¾
        for idx_year, year in enumerate(city_df['å¹´ä»½']):
            plt.annotate(str(year), (data_density.iloc[idx_year], digital_share.iloc[idx_year]),
                         textcoords="offset points", xytext=(5, 5), fontsize=8)

plt.xlabel('æ•°æ®å¯†åº¦ (TB/åäº¿GDP)', fontsize=14)
plt.ylabel('æ•°å­—ç»æµå GDPæ¯”é‡ (%)', fontsize=14)
plt.title('ä»£è¡¨æ€§åŸå¸‚æ•°æ®è¦ç´ å‘å±•è·¯å¾„æ¼”å˜ (2019-2023)', fontsize=16, fontweight='bold', pad=20)
plt.legend(loc='best', fontsize=10)
plt.grid(True, alpha=0.3)

# æ·»åŠ å›¾ä¾‹è¯´æ˜
plt.text(0.02, 0.98, 'èµ·ç‚¹ (2019å¹´)', transform=plt.gca().transAxes,
         fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))
plt.text(0.02, 0.94, 'ç»ˆç‚¹ (2023å¹´)', transform=plt.gca().transAxes,
         fontsize=10, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))

plt.tight_layout()
plt.savefig('å›¾10_å‘å±•è·¯å¾„å›¾.png', dpi=300, bbox_inches='tight')
plt.show()

# ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
df_2023.to_csv('å¤§æ¹¾åŒºæ•°æ®è¦ç´ åˆ†æç»“æœ_2023.csv', index=False, encoding='utf-8-sig')
print(f"\nè¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜è‡³: å¤§æ¹¾åŒºæ•°æ®è¦ç´ åˆ†æç»“æœ_2023.csv")
print(f"ğŸ“Š æœ€ä¼˜èšç±»æ•°: {optimal_k}ç±» (è½®å»“ç³»æ•°: {max(sil_scores):.3f})")
print(f"ğŸ™ï¸ èšç±»ç»“æœ:")
for cluster in cluster_summary:
    print(f"  â€¢ {cluster['ç±»åˆ«åç§°']}: {cluster['åŸå¸‚æ•°é‡']}ä¸ªåŸå¸‚")
print(f"ğŸ“Š åˆ¤åˆ«åˆ†æå‡†ç¡®ç‡: {accuracy:.1%}") if 'accuracy' in locals() else None
